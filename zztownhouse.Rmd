---
title: "melbourneTown"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

```{r}


melHouseStart <- read.csv("~/Desktop/Melbourne data project/melbourne-housing-market/Melbourne_housing_FULL.csv")


library(tidyverse)

#----------------

melHouseStart <- as.tibble(melHouseStart)

# check the structure
str(melHouseStart)

```

## Including Plots

You can also embed plots, for example:

```{r}

# Fix data types
# changing Distance to numeric, Propertycount to numeric, Date to date /d/m/y date format


melHouseStart$Distance <- as.numeric(as.character(melHouseStart$Distance))

melHouseStart$Propertycount <- as.numeric(as.character(melHouseStart$Propertycount))

melHouseStart$Date <- as.Date(melHouseStart$Date, "%d/%m/%Y")

#confirming the changes
glimpse(melHouseStart)


```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}

# Adding month and year to the columns & correcting spelling on 

melHouse <- melHouseStart %>%
  mutate(Month = as.factor(strftime(Date, "%m")),
         Year = as.factor(strftime(Date, "%y"))) %>%
  rename("Lat" = "Lattitude", "Long" = "Longtitude", "Region" = "Regionname", "PropertyCount" = "Propertycount")

glimpse(melHouse) # verifying that the columns were created and renamed

```

```{r}
# Finding the descriptive statistics

summary(melHouse) # descriptive statistics with out standard deviation

```

```{r}

# getting standard deviation for each variable

melHouseSD <- melHouse %>%
  summarise_all(funs(sd(., na.rm=TRUE)))

glimpse(melHouseSD) # getting a clean print out of the SD values

```

```{r}
# checking NA values

library(Amelia)
missmap(melHouse) # visualise missing values

```

```{r}

# Actual NA values followed by percentage of NA values

colSums(is.na(melHouse))

# Finding the percentage of NA values per variable

colMeans(is.na(melHouse))*100

```

```{r}

# inspecting rooms vs bedroom2 as Bedroom2 was scraped from a different source

melHouse %>%
  select(Rooms, Bedroom2)

# inspecting the rows that are not equal to each other
melHouse %>%
  select(Rooms, Bedroom2) %>%
  filter(Rooms != Bedroom2)

# getting a count of how many rows contain the same values
melHouse %>%
  select(Rooms, Bedroom2) %>%
  count(Rooms == Bedroom2)

# checking the correlation
cor.test(melHouse$Rooms, melHouse$Bedroom2)

# Drop Bedroom2 due to multicolinearity issues

```

```{r}

(melExtended <- select(melHouse, - SellerG, - Lat, - Long, 
                           - Method, - Suburb, -Postcode, -Address, 
                           - PropertyCount, - CouncilArea, - Bedroom2, - Date))

```

```{r}

# Getting rid of any NA values

melExtended <- na.omit(melExtended)
str(melExtended)
summary(melExtended)

```

```{r}

# YearBuilt 2019 does not exist yet, might be a presale, but will be removed

# seeing how many homes claim to be built after 2018 & deleting anything greater than 2018

melExtended %>%
  select(YearBuilt) %>%
  filter(YearBuilt > 2018)

melExtended <- melExtended %>%
  filter(YearBuilt <= 2018)

summary(melExtended$YearBuilt) # veridying max is no greater than 2018

```

```{r}

# checking for homes built before 1800

melExtended %>%
  select(YearBuilt) %>%
  filter(YearBuilt < 1800)

# unsure of anything built in 1196, therefore it is being removed

melExtended <- melExtended %>%
  filter(YearBuilt > 1800)

summary(melExtended$YearBuilt)

```
```{r}
# Turning YearBuilt into a range of factors
# 1 = 1800 -1899
# 2 = 1900 - 1949
# 3 = 1950 - 1999
# 4 = 1999 - 2018


# adding column BuildYear for range of years built
melExtended$BuildYear  = 0

melExtended$BuildYear[melExtended$YearBuilt <= 1899] = 1
melExtended$BuildYear[melExtended$YearBuilt >= 1900 & melExtended$YearBuilt<= 1949] = 2
melExtended$BuildYear[melExtended$YearBuilt >= 1950 & melExtended$YearBuilt<= 1999] = 3
melExtended$BuildYear[melExtended$YearBuilt >= 2000 & melExtended$YearBuilt<= 2018] = 4

glimpse(melExtended) #verifying BuildYear is created
```

```{r}

# turn BuildYear into a factor
melExtended$BuildYear <- as.factor(melExtended$BuildYear)

# drop year built

melExtended <- melExtended %>%
  select(- YearBuilt)

glimpse(melExtended)

```

```{r}

# Eliminate any home where it is claimed it has more bathrooms than rooms

melExtended <- melExtended %>%
    filter(Bathroom < Rooms)

# Elinimate any home claiming to have 0 bathrooms or more and 5 (Reason to believe more than 5 is a building of units sold as a whole in most cases)
melExtended <- melExtended %>%
  filter(Bathroom > 0 & Bathroom <= 5)

# Eliminate any home with 0 Landsize
melExtended<- melExtended %>%
    filter(Landsize > 0)

# using this link https://www.smh.com.au/business/melbournes-apartment-sizes-face-more-scrutiny-20150414-1mkuj4.html
# setting a lowest value of 40, this 10 square meters lower than 2002 requirements

melExtended <- melExtended %>%
    filter(Landsize >= 40)


# eliminating any lot over 1500

melExtended <- melExtended %>%
  filter(Landsize < 1500)

melExtended <- melExtended %>%
  filter(BuildingArea >= 40) # same as above with relation to landsize

summary(melExtended)


```

```{r}

# Eliminate any property where building area is > land size

melExtended <- melExtended %>%
  filter(BuildingArea < Landsize)

summary(melExtended)

```

```{r}

# remove categorical to get numeric only

melExCorr <- melExtended %>%
   select(- Type, - Month, - Year, - Region, - BuildYear)

library(corrplot)
corrplot(cor(melExCorr), method = 'number')

# Nothing as of now will be dropped as there is no correlation above 80

```

```{r}

#separate into 4 groups, all, house, unit, townhouse (h/u/t) while dropping 'Type'


mxapartment <- melExtended %>%
   filter(Type == 'u') %>%
   select(- Type)

mxhouse <- melExtended %>%
    filter(Type == 'h') %>%
    select(- Type)

mxtownhouse <- melExtended %>%
    filter(Type == 't') %>%
    select( - Type)

# the forth group will remain using melExtended

```

```{r}

# checking strength of variables & models adjusted R-squared prior to inspecting and normalizing variables

# note these models are useless if they do not meet the assumptions

set.seed(99)

(summary(lm(Price ~., data = mxapartment))) 
(summary(lm(Price ~., data = mxtownhouse))) 
(summary(lm(Price ~., data = mxhouse))) 

```


```{r}


melHouseStart <- read.csv("~/Desktop/Melbourne data project/melbourne-housing-market/Melbourne_housing_FULL.csv")


library(tidyverse)

#----------------

melHouseStart <- as.tibble(melHouseStart)

# check the structure
str(melHouseStart)

```

## Including Plots

You can also embed plots, for example:

```{r}

# Fix data types
# changing Distance to numeric, Propertycount to numeric, Date to date /d/m/y date format


melHouseStart$Distance <- as.numeric(as.character(melHouseStart$Distance))

melHouseStart$Propertycount <- as.numeric(as.character(melHouseStart$Propertycount))

melHouseStart$Date <- as.Date(melHouseStart$Date, "%d/%m/%Y")

#confirming the changes
glimpse(melHouseStart)


```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}

# Adding month and year to the columns & correcting spelling on 

melHouse <- melHouseStart %>%
  mutate(Month = as.factor(strftime(Date, "%m")),
         Year = as.factor(strftime(Date, "%y"))) %>%
  rename("Lat" = "Lattitude", "Long" = "Longtitude", "Region" = "Regionname", "PropertyCount" = "Propertycount")

glimpse(melHouse) # verifying that the columns were created and renamed

```

```{r}
# Finding the descriptive statistics

summary(melHouse) # descriptive statistics with out standard deviation

```

```{r}

# getting standard deviation for each variable

melHouseSD <- melHouse %>%
  summarise_all(funs(sd(., na.rm=TRUE)))

glimpse(melHouseSD) # getting a clean print out of the SD values

```

```{r}
# checking NA values

library(Amelia)
missmap(melHouse) # visualise missing values

```

```{r}

# Actual NA values followed by percentage of NA values

colSums(is.na(melHouse))

# Finding the percentage of NA values per variable

colMeans(is.na(melHouse))*100

```

```{r}

# inspecting rooms vs bedroom2 as Bedroom2 was scraped from a different source

melHouse %>%
  select(Rooms, Bedroom2)

# inspecting the rows that are not equal to each other
melHouse %>%
  select(Rooms, Bedroom2) %>%
  filter(Rooms != Bedroom2)

# getting a count of how many rows contain the same values
melHouse %>%
  select(Rooms, Bedroom2) %>%
  count(Rooms == Bedroom2)

# checking the correlation
cor.test(melHouse$Rooms, melHouse$Bedroom2)

# Drop Bedroom2 due to multicolinearity issues

```

```{r}

(melExtended <- select(melHouse, - SellerG, - Lat, - Long, 
                           - Method, - Suburb, -Postcode, -Address, 
                           - PropertyCount, - CouncilArea, - Bedroom2, - Date))

```

```{r}

# Getting rid of any NA values

melExtended <- na.omit(melExtended)
str(melExtended)
summary(melExtended)

```

```{r}

# YearBuilt 2019 does not exist yet, might be a presale, but will be removed

# seeing how many homes claim to be built after 2018 & deleting anything greater than 2018

melExtended %>%
  select(YearBuilt) %>%
  filter(YearBuilt > 2018)

melExtended <- melExtended %>%
  filter(YearBuilt <= 2018)

summary(melExtended$YearBuilt) # veridying max is no greater than 2018

```

```{r}

# checking for homes built before 1800

melExtended %>%
  select(YearBuilt) %>%
  filter(YearBuilt < 1800)

# unsure of anything built in 1196, therefore it is being removed

melExtended <- melExtended %>%
  filter(YearBuilt > 1800)

summary(melExtended$YearBuilt)

```
```{r}
# Turning YearBuilt into a range of factors
# 1 = 1800 -1899
# 2 = 1900 - 1949
# 3 = 1950 - 1999
# 4 = 1999 - 2018


# adding column BuildYear for range of years built
melExtended$BuildYear  = 0

melExtended$BuildYear[melExtended$YearBuilt <= 1899] = 1
melExtended$BuildYear[melExtended$YearBuilt >= 1900 & melExtended$YearBuilt<= 1949] = 2
melExtended$BuildYear[melExtended$YearBuilt >= 1950 & melExtended$YearBuilt<= 1999] = 3
melExtended$BuildYear[melExtended$YearBuilt >= 2000 & melExtended$YearBuilt<= 2018] = 4

glimpse(melExtended) #verifying BuildYear is created
```

```{r}

# turn BuildYear into a factor
melExtended$BuildYear <- as.factor(melExtended$BuildYear)

# drop year built

melExtended <- melExtended %>%
  select(- YearBuilt)

glimpse(melExtended)

```

```{r}

# Eliminate any home where it is claimed it has more bathrooms than rooms

melExtended <- melExtended %>%
    filter(Bathroom < Rooms)

# Elinimate any home claiming to have 0 bathrooms or more and 5 (Reason to believe more than 5 is a building of units sold as a whole in most cases)
melExtended <- melExtended %>%
  filter(Bathroom > 0 & Bathroom <= 5)

# Eliminate any home with 0 Landsize
melExtended<- melExtended %>%
    filter(Landsize > 0)

# using this link https://www.smh.com.au/business/melbournes-apartment-sizes-face-more-scrutiny-20150414-1mkuj4.html
# setting a lowest value of 40, this 10 square meters lower than 2002 requirements

melExtended <- melExtended %>%
    filter(Landsize >= 40)


# eliminating any lot over 1500

melExtended <- melExtended %>%
  filter(Landsize < 1500)

melExtended <- melExtended %>%
  filter(BuildingArea >= 40) # same as above with relation to landsize

summary(melExtended)


```

```{r}

# Eliminate any property where building area is > land size

melExtended <- melExtended %>%
  filter(BuildingArea < Landsize)

summary(melExtended)

```

```{r}

# remove categorical to get numeric only

melExCorr <- melExtended %>%
   select(- Type, - Month, - Year, - Region, - BuildYear)

library(corrplot)
corrplot(cor(melExCorr), method = 'number')

# Nothing as of now will be dropped as there is no correlation above 80

```

```{r}

#separate into 4 groups, all, house, unit, townhouse (h/u/t) while dropping 'Type'


mxapartment <- melExtended %>%
   filter(Type == 'u') %>%
   select(- Type)

mxhouse <- melExtended %>%
    filter(Type == 'h') %>%
    select(- Type)

mxtownhouse <- melExtended %>%
    filter(Type == 't') %>%
    select( - Type)

# the forth group will remain using melExtended

```

```{r}

summary(mxtownhouse)

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}

# applying Log10 to variables in need for houses only preparing for regressions
# loading packeages needed

library(caret)
library(psych)
library(glmnet)
library(mlbench)

```

```{r}

# inpspecting Price variable and applying log10 transformation

# price
hist(mxtownhouse$Price)

```

```{r}

plot(mxtownhouse$Price)

```

```{r}

# price

tp1 <- log10(mxtownhouse$Price)
hist(tp1, main = 'Histogram of Log10(Price)', xlab = 'Log10(Price)')

```

```{r}

# price
plot(tp1)

```

```{r}

# Rooms
summary(mxtownhouse$Rooms) 
hist(mxtownhouse$Rooms) 


```

```{r}

```

```{r}


```

```{r}

```

```{r}
# Distance

hist(mxtownhouse$Distance) #skewed

```

```{r}
# check Distance for min distance to determine if it can be log10 without addition
summary(mxtownhouse)
```

```{r}
td1 <- mxtownhouse$Distance  
td2 <- log10(td1)
hist(td2)


```

```{r}
qqnorm(y = tp1, x = td2) 
```

```{r}
plot(td2)
```

```{r}
# Bathrooms

hist(mxtownhouse$Bathroom)

```

```{r}

```

```{r}
# Parking spots
hist(mxtownhouse$Car) 

```

```{r}
 
```

```{r}
# LandSize

hist(mxtownhouse$Landsize) #skewed


```

```{r}

tland <- log10(mxtownhouse$Landsize)
hist(tland) # still seems skewed

```

```{r}

#builing Area
hist(mxtownhouse$BuildingArea)

```

```{r}
tba <- log10(mxtownhouse$BuildingArea)
hist(tba, main = 'Histogram Log10(BuildingArea)') # better
```

```{r}
#correlation, scatterplots, histograms
pairs.panels(mxapartment) # prior to setting anything to log10
```

```{r}
# pretesting simple regression with log10 variables, just to get an idea of the best fitting model
# before separating into folds for training and testing

set.seed(99)
# only price is logged

totrial <- lm(log10(Price) ~., data = mxtownhouse)
summary(totrial) 
```

```{r}

# using a log10 on Price and Distance

tdata <- mxtownhouse
tdata$Distance <- mxtownhouse$Distance 
tdata$Distance <- log10(tdata$Distance)

totrial2 <- lm(log10(Price) ~., data = tdata)
summary(totrial2) # improves on above model
```

```{r}
tdata2 <- tdata
tdata2$BuildingArea <- log10(tdata2$BuildingArea)

totrial3 <- lm(log10(Price) ~., data = tdata2)
summary(totrial3) # miniscule improvement

```

```{r}
# testing selection methods

# backwards selection
summary(step(totrial3, direction = 'backward', trace = 0)) 

```

```{r}
# forwards selection
summary(step(totrial3, direction = 'forward', trace = 0)) 
```

```{r}
# stepwise selection (both)
summary(step(totrial3, direction = 'both', trace = 0))

# forward selection in minimally more efficient here
# forward uses all 10 variables,
# backwards and step use 5 variables
```

```{r}

# Log10 of Price into data prior to setting training and testing sets

# log10 to Price

tdata3 <- tdata2
tdata3$Price <- log10(tdata2$Price)
glimpse(tdata3) # verifying it was logged

```

```{r}
#correlation with log10 features

pairs.panels(tdata3, cex.cor = 2) # with log10 data, cex.cor =2 makes correlation #'s larger

```

```{r}
# setting up training and test sets

set.seed(99)

# creating the index for the split to occur
tIndex <- sample(2, nrow(tdata3), replace = T, prob = c(0.7, 0.3))


toTrain <- tdata3[tIndex == 1, ] # create training set, 70%
toTest <- tdata3[tIndex == 2, ] # create test set, 30%

library(caret)

# creating cross validation, 10 fold, ran 5 times
# control parameters

tControls <- trainControl(method = 'repeatedcv', number = 10,  repeats = 5, verboseIter = F) #verboseIter shows iterations running on screen
```

```{r}

# Linear Regression

tLinTrain <- train(Price ~., toTrain, method = 'lm', trControl = tControls)
tLinTrain$results
summary(tLinTrain)

```

```{r}
plot(tLinTrain$finalModel)

#seems to be an issue at cooks distance

```

```{r}
set.seed(99)


library(glmnet) 
# Setting up Ridge regression

tRidge <- train(Price ~., 
                  toTrain, 
                  method = 'glmnet', 
                  tuneGrid = expand.grid(alpha = 0,
                                         lambda = seq(0.0001, 1, length = 5)),
                  trControl = tControls)

```

```{r}
plot(tRidge) # Showing RMSE responding to increase in Lambda values
```

```{r}
print(tRidge)
```

```{r}
# plot showing importance of variables for Ridge Regression

plot(varImp(tRidge, scale = F)) 
```

```{r}

# Lasso Regression

set.seed(99)

tLasso <- train(Price ~., 
                         toTrain, 
                         method = 'glmnet', 
                         tuneGrid = expand.grid(alpha = 1,
                                                lambda = seq(0.0001, 1, length = 5)),
                         trControl = tControls)
```

```{r}
plot(tLasso) # lowest lambda is best for lasso as well
```

```{r}
plot(varImp(tLasso, scale = F)) # importance level of variables for Lasso
```

```{r}
plot(tLasso$finalModel, xvar = 'dev', label = T) # 4 variables explain 60%
```

```{r}
# Elastic Net

set.seed(99)

tElastic <- train(Price ~., 
                         toTrain, 
                         method = 'glmnet', 
                         tuneGrid = expand.grid(alpha = seq(0, 1, length =10),
                                                lambda = seq(0.0001, 1, length = 5)),
                         trControl = tControls)

```

```{r}
# best fit for Lamba and Alpha graph

plot(tElastic) #lambda is coloured lines, alpha along bottom, run again with length 3 for alpha for clarity

```

```{r}
tElastic$bestTune #finding optimal lambda & alpha values in a separate way

```

```{r}
# Comparing models - MAE, RMSE, Rsquared

tModelList <- list(Ridge = tRidge, Lasso = tLasso, Elastic = tElastic)
tResample <- resamples(tModelList)
summary(tResample)

```

```{r}
# Predictions with Training & Test data
# all models performed closely however the  Elastic net regression have the best mean RMSE
# So Elastic will be used here, as RMSE is the main measure of this study


# training data prediction & RMSE
tPredict1 <- predict(tElastic, toTrain)
sqrt(mean((toTrain$Price - tPredict1)^2)) # RMSE - lower the better
```

```{r}
# Test data prediction RMSE
tPredict2 <- predict(tElastic, toTest)
sqrt(mean((toTest$Price - tPredict2)^2))

```

```{r}
# comparing accuracy

#comparing actual vs predicted values 
tCompare <- cbind(actual = toTest$Price, tPredict2) # combining actual and predicted

head(tCompare)

```

```{r}
# calculating accuracy
mean (apply(tCompare, 1, min)/apply(tCompare, 1, max))

# The model is very accurate
```

```{r}

# For the Log10(Distance variable) the percent increase is computed as follows

# (Distance1/ Distance2) ^ Distance Coefficient

```

```{r}
# Decision Tree

library(rpart)
set.seed(99)

toTree <- rpart(Price ~., data = toTrain, method = "anova")
summary(toTree)

```

```{r}
#RMSE for Decision Tree Training set

toTreeP1 <- predict(toTree, toTrain )
RMSE(toTreeP1, toTrain$Price)
```


```{r}
# done on test data

toTreeP2 <- predict(toTree, toTest )
RMSE(toTreeP2, toTest$Price)
```

```{r}
# Using random forest

library(randomForest)

set.seed(99)

tForestTry <- randomForest(Price~., data = toTrain)
attributes(tForestTry) # gives a list of attributes that can go after $
tForestTry$importance # gives importance of each variable

```

```{r}

tForestTry$type # regression

```

```{r}
plot(tForestTry) # graph on error as it reduces and becomes constant for number of trees

```

```{r}
# histogram of the number of nodes per tree
hist(treesize(tForestTry),
     main = '# of Nodes per Tree',
     col = 'azure2')

```

```{r}
# graphical display of ForestTry$importance
varImpPlot(tForestTry, main = 'Variable Importance', col = 'slateblue4') 
```

```{r}
# Number of times a variable appeared in random forest, printed in relation to the variable column number
varUsed(tForestTry)
```

```{r}
# print out of ForestTry and results
tForestTry
```

```{r}
# Prediction for RMSE on Train data

tPreTry <- predict(tForestTry, toTrain) # default
(RMSE(tPreTry, toTrain$Price))       # default
```

```{r}
# prediction RMSE for Test data

tPreTest <- predict(tForestTry, toTest)  
(RMSE(tPreTest, toTest$Price))
```

```{r}

# attempting to tune the Random Forest, finding best mtry and oob error

toTuneForest<- tuneRF(y = toTrain$Price, x = toTrain[,-1], 
                    mtryStart = 3, #using default, same as above to start
                    ntreeTry = 350, # on plot error seems to stabilize a 350
                    stepFactor = 1,
                    improve = 0.05,
                    trace = T,
                    plot = T,
                    doBest = T)
```

```{r}

tForest <- randomForest(Price ~., data = toTrain,
                          ntree = 500,
                           # default for regression
                          importance = T)

tForest

```

```{r}

plot(tForest)

```

```{r}

```

```{r}

# getting RMSE for training data

tForestP1 <- predict(tForest, toTrain)

plot(tForestP1)

```

```{r}
# RMSE for training data

RMSE(tForestP1, toTrain$Price)

```

```{r}
# plot for Test data

tForestP2 <- predict(tForest, toTest)
plot(tForestP2)

```

```{r}

# RMSE for Test data
RMSE(tForestP2, toTest$Price)

```

```{r}

```

```{r}

```
