---
title: "melbapart"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




```{r}

melHouseStart <- read.csv("~/Desktop/Melbourne data project/melbourne-housing-market/Melbourne_housing_FULL.csv")


library(tidyverse)

#----------------

melHouseStart <- as.tibble(melHouseStart)

# check the structure
str(melHouseStart)

```

## Including Plots

You can also embed plots, for example:

```{r}

# Fix data types
# changing Distance to numeric, Propertycount to numeric, Date to date /d/m/y date format


melHouseStart$Distance <- as.numeric(as.character(melHouseStart$Distance))

melHouseStart$Propertycount <- as.numeric(as.character(melHouseStart$Propertycount))

melHouseStart$Date <- as.Date(melHouseStart$Date, "%d/%m/%Y")

#confirming the changes
glimpse(melHouseStart)


```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}

# Adding month and year to the columns & correcting spelling on 

melHouse <- melHouseStart %>%
  mutate(Month = as.factor(strftime(Date, "%m")),
         Year = as.factor(strftime(Date, "%y"))) %>%
  rename("Lat" = "Lattitude", "Long" = "Longtitude", "Region" = "Regionname", "PropertyCount" = "Propertycount")

glimpse(melHouse) # verifying that the columns were created and renamed

```

```{r}
# Finding the descriptive statistics

summary(melHouse) # descriptive statistics with out standard deviation

```

```{r}

# getting standard deviation for each variable

melHouseSD <- melHouse %>%
  summarise_all(funs(sd(., na.rm=TRUE)))

glimpse(melHouseSD) # getting a clean print out of the SD values

```

```{r}
# checking NA values

library(Amelia)
missmap(melHouse) # visualise missing values

```

```{r}

# Actual NA values followed by percentage of NA values

colSums(is.na(melHouse))

# Finding the percentage of NA values per variable

colMeans(is.na(melHouse))*100

```

```{r}

# inspecting rooms vs bedroom2 as Bedroom2 was scraped from a different source

melHouse %>%
  select(Rooms, Bedroom2)

# inspecting the rows that are not equal to each other
melHouse %>%
  select(Rooms, Bedroom2) %>%
  filter(Rooms != Bedroom2)

# getting a count of how many rows contain the same values
melHouse %>%
  select(Rooms, Bedroom2) %>%
  count(Rooms == Bedroom2)

# checking the correlation
cor.test(melHouse$Rooms, melHouse$Bedroom2)

# Drop Bedroom2 due to multicolinearity issues

```

```{r}

(melExtended <- select(melHouse, - SellerG, - Lat, - Long, 
                           - Method, - Suburb, -Postcode, -Address, 
                           - PropertyCount, - CouncilArea, - Bedroom2, - Date))

```

```{r}

# Getting rid of any NA values

melExtended <- na.omit(melExtended)
str(melExtended)
summary(melExtended)

```

```{r}

# YearBuilt 2019 does not exist yet, might be a presale, but will be removed

# seeing how many homes claim to be built after 2018 & deleting anything greater than 2018

melExtended %>%
  select(YearBuilt) %>%
  filter(YearBuilt > 2018)

melExtended <- melExtended %>%
  filter(YearBuilt <= 2018)

summary(melExtended$YearBuilt) # veridying max is no greater than 2018

```

```{r}

# checking for homes built before 1800

melExtended %>%
  select(YearBuilt) %>%
  filter(YearBuilt < 1800)

# unsure of anything built in 1196, therefore it is being removed

melExtended <- melExtended %>%
  filter(YearBuilt > 1800)

summary(melExtended$YearBuilt)

```
```{r}
# Turning YearBuilt into a range of factors
# 1 = 1800 -1899
# 2 = 1900 - 1949
# 3 = 1950 - 1999
# 4 = 1999 - 2018


# adding column BuildYear for range of years built
melExtended$BuildYear  = 0

melExtended$BuildYear[melExtended$YearBuilt <= 1899] = 1
melExtended$BuildYear[melExtended$YearBuilt >= 1900 & melExtended$YearBuilt<= 1949] = 2
melExtended$BuildYear[melExtended$YearBuilt >= 1950 & melExtended$YearBuilt<= 1999] = 3
melExtended$BuildYear[melExtended$YearBuilt >= 2000 & melExtended$YearBuilt<= 2018] = 4

glimpse(melExtended) #verifying BuildYear is created
```

```{r}

# turn BuildYear into a factor
melExtended$BuildYear <- as.factor(melExtended$BuildYear)

# drop year built

melExtended <- melExtended %>%
  select(- YearBuilt)

glimpse(melExtended)

```

```{r}

# Eliminate any home where it is claimed it has more bathrooms than rooms

melExtended <- melExtended %>%
    filter(Bathroom < Rooms)

# Elinimate any home claiming to have 0 bathrooms or more and 5 (Reason to believe more than 5 is a building of units sold as a whole in most cases)
melExtended <- melExtended %>%
  filter(Bathroom > 0 & Bathroom <= 5)

# Eliminate any home with 0 Landsize
melExtended<- melExtended %>%
    filter(Landsize > 0)

# using this link https://www.smh.com.au/business/melbournes-apartment-sizes-face-more-scrutiny-20150414-1mkuj4.html
# setting a lowest value of 40, this 10 square meters lower than 2002 requirements

melExtended <- melExtended %>%
    filter(Landsize >= 40)


# eliminating any lot over 1500

melExtended <- melExtended %>%
  filter(Landsize < 1500)

melExtended <- melExtended %>%
  filter(BuildingArea >= 40) # same as above with relation to landsize

summary(melExtended)


```

```{r}

# Eliminate any property where building area is > land size

melExtended <- melExtended %>%
  filter(BuildingArea < Landsize)

summary(melExtended)

```

```{r}

# remove categorical to get numeric only

melExCorr <- melExtended %>%
   select(- Type, - Month, - Year, - Region, - BuildYear)

library(corrplot)
corrplot(cor(melExCorr), method = 'number')

# Nothing as of now will be dropped as there is no correlation above 80

```

```{r}

#separate into 4 groups, all, house, unit, townhouse (h/u/t) while dropping 'Type'


mxapartment <- melExtended %>%
   filter(Type == 'u') %>%
   select(- Type)

mxhouse <- melExtended %>%
    filter(Type == 'h') %>%
    select(- Type)

mxtownhouse <- melExtended %>%
    filter(Type == 't') %>%
    select( - Type)

# the forth group will remain using melExtended

```

```{r}

# checking strength of variables & models adjusted R-squared prior to inspecting and normalizing variables

# note these models are useless if they do not meet the assumptions

set.seed(99)

(summary(lm(Price ~., data = mxapartment))) 
(summary(lm(Price ~., data = mxtownhouse))) 
(summary(lm(Price ~., data = mxhouse))) 

```


```{r}


melHouseStart <- read.csv("~/Desktop/Melbourne data project/melbourne-housing-market/Melbourne_housing_FULL.csv")


library(tidyverse)

#----------------

melHouseStart <- as.tibble(melHouseStart)

# check the structure
str(melHouseStart)

```

## Including Plots

You can also embed plots, for example:

```{r}

# Fix data types
# changing Distance to numeric, Propertycount to numeric, Date to date /d/m/y date format


melHouseStart$Distance <- as.numeric(as.character(melHouseStart$Distance))

melHouseStart$Propertycount <- as.numeric(as.character(melHouseStart$Propertycount))

melHouseStart$Date <- as.Date(melHouseStart$Date, "%d/%m/%Y")

#confirming the changes
glimpse(melHouseStart)


```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}

# Adding month and year to the columns & correcting spelling on 

melHouse <- melHouseStart %>%
  mutate(Month = as.factor(strftime(Date, "%m")),
         Year = as.factor(strftime(Date, "%y"))) %>%
  rename("Lat" = "Lattitude", "Long" = "Longtitude", "Region" = "Regionname", "PropertyCount" = "Propertycount")

glimpse(melHouse) # verifying that the columns were created and renamed

```

```{r}
# Finding the descriptive statistics

summary(melHouse) # descriptive statistics with out standard deviation

```

```{r}

# getting standard deviation for each variable

melHouseSD <- melHouse %>%
  summarise_all(funs(sd(., na.rm=TRUE)))

glimpse(melHouseSD) # getting a clean print out of the SD values

```

```{r}
# checking NA values

library(Amelia)
missmap(melHouse) # visualise missing values

```

```{r}

# Actual NA values followed by percentage of NA values

colSums(is.na(melHouse))

# Finding the percentage of NA values per variable

colMeans(is.na(melHouse))*100

```

```{r}

# inspecting rooms vs bedroom2 as Bedroom2 was scraped from a different source

melHouse %>%
  select(Rooms, Bedroom2)

# inspecting the rows that are not equal to each other
melHouse %>%
  select(Rooms, Bedroom2) %>%
  filter(Rooms != Bedroom2)

# getting a count of how many rows contain the same values
melHouse %>%
  select(Rooms, Bedroom2) %>%
  count(Rooms == Bedroom2)

# checking the correlation
cor.test(melHouse$Rooms, melHouse$Bedroom2)

# Drop Bedroom2 due to multicolinearity issues

```

```{r}

(melExtended <- select(melHouse, - SellerG, - Lat, - Long, 
                           - Method, - Suburb, -Postcode, -Address, 
                           - PropertyCount, - CouncilArea, - Bedroom2, - Date))

```

```{r}

# Getting rid of any NA values

melExtended <- na.omit(melExtended)
str(melExtended)
summary(melExtended)

```

```{r}

# YearBuilt 2019 does not exist yet, might be a presale, but will be removed

# seeing how many homes claim to be built after 2018 & deleting anything greater than 2018

melExtended %>%
  select(YearBuilt) %>%
  filter(YearBuilt > 2018)

melExtended <- melExtended %>%
  filter(YearBuilt <= 2018)

summary(melExtended$YearBuilt) # veridying max is no greater than 2018

```

```{r}

# checking for homes built before 1800

melExtended %>%
  select(YearBuilt) %>%
  filter(YearBuilt < 1800)

# unsure of anything built in 1196, therefore it is being removed

melExtended <- melExtended %>%
  filter(YearBuilt > 1800)

summary(melExtended$YearBuilt)

```
```{r}
# Turning YearBuilt into a range of factors
# 1 = 1800 -1899
# 2 = 1900 - 1949
# 3 = 1950 - 1999
# 4 = 1999 - 2018


# adding column BuildYear for range of years built
melExtended$BuildYear  = 0

melExtended$BuildYear[melExtended$YearBuilt <= 1899] = 1
melExtended$BuildYear[melExtended$YearBuilt >= 1900 & melExtended$YearBuilt<= 1949] = 2
melExtended$BuildYear[melExtended$YearBuilt >= 1950 & melExtended$YearBuilt<= 1999] = 3
melExtended$BuildYear[melExtended$YearBuilt >= 2000 & melExtended$YearBuilt<= 2018] = 4

glimpse(melExtended) #verifying BuildYear is created
```

```{r}

# turn BuildYear into a factor
melExtended$BuildYear <- as.factor(melExtended$BuildYear)

# drop year built

melExtended <- melExtended %>%
  select(- YearBuilt)

glimpse(melExtended)

```

```{r}

# Eliminate any home where it is claimed it has more bathrooms than rooms

melExtended <- melExtended %>%
    filter(Bathroom < Rooms)

# Elinimate any home claiming to have 0 bathrooms or more and 5 (Reason to believe more than 5 is a building of units sold as a whole in most cases)
melExtended <- melExtended %>%
  filter(Bathroom > 0 & Bathroom <= 5)

# Eliminate any home with 0 Landsize
melExtended<- melExtended %>%
    filter(Landsize > 0)

# using this link https://www.smh.com.au/business/melbournes-apartment-sizes-face-more-scrutiny-20150414-1mkuj4.html
# setting a lowest value of 40, this 10 square meters lower than 2002 requirements

melExtended <- melExtended %>%
    filter(Landsize >= 40)


# eliminating any lot over 1500

melExtended <- melExtended %>%
  filter(Landsize < 1500)

melExtended <- melExtended %>%
  filter(BuildingArea >= 40) # same as above with relation to landsize

summary(melExtended)


```

```{r}

# Eliminate any property where building area is > land size

melExtended <- melExtended %>%
  filter(BuildingArea < Landsize)

summary(melExtended)

```

```{r}

# remove categorical to get numeric only

melExCorr <- melExtended %>%
   select(- Type, - Month, - Year, - Region, - BuildYear)

library(corrplot)
corrplot(cor(melExCorr), method = 'number')

# Nothing as of now will be dropped as there is no correlation above 80

```

```{r}

#separate into 4 groups, all, house, unit, townhouse (h/u/t) while dropping 'Type'


mxapartment <- melExtended %>%
   filter(Type == 'u') %>%
   select(- Type)

mxhouse <- melExtended %>%
    filter(Type == 'h') %>%
    select(- Type)

mxtownhouse <- melExtended %>%
    filter(Type == 't') %>%
    select( - Type)

# the forth group will remain using melExtended

```


```{r}
# Separate apartment section from data, original mouse section must be ran first before this section will work
summary(mxapartment)

```
```{r}

# applying Log10 to variables in need for houses only preparing for regressions
# loading packeages needed

library(caret)
library(psych)
library(glmnet)
library(mlbench)

```

```{r}

# inpspecting Price variable and applying log10 transformation

# price
hist(mxapartment$Price)

```

```{r}

plot(mxapartment$Price)

```

```{r}

# price

ap1 <- log10(mxapartment$Price)
hist(ap1, main = 'Histogram of Log10(Price)', xlab = 'Log10(Price)')

```

```{r}

# price
plot(ap1)

```

```{r}

# Rooms
summary(mxapartment$Rooms) #min is 2 no need to add 1
hist(mxapartment$Rooms) 


```

```{r}

```

```{r}

```

```{r}

```

```{r}
# Distance

hist(mxapartment$Distance) #skewed

```

```{r}

ad1 <- mxapartment$Distance + 2 # adding 2 so the log10 has no 0 values
ad2 <- log10(ad1)
hist(ad2, main = 'Histogram of Distance + 2, Log10(Distance)', col = 'lightblue')


```

```{r}
qqnorm(y = ap1, x = ad2) 
```

```{r}
plot(ad2)
```

```{r}
# Bathrooms

hist(mxapartment$Bathroom)

```

```{r}

```

```{r}
# Parking spots
hist(mxapartment$Car) 

```

```{r}
 
```

```{r}
# LandSize

hist(mxapartment$Landsize) #skewed


```

```{r}

aland <- log10(mxapartment$Landsize)
hist(aland) # still bimodal? need to use non-parametric decision tree and random forest for results, Gaussien model cannot be fitted.

```

```{r}

#builing Area
hist(mxapartment$BuildingArea)

```

```{r}
aba <- log10(mxapartment$BuildingArea)
hist(aba, main = 'Histogram Log10(BuildingArea)') # looks better
```

```{r}
#correlation, scatterplots, histograms
pairs.panels(mxapartment) # prior to setting anything to log10
```

```{r}
# pretesting simple regression with log10 variables, just to get an idea of the best fitting model
# before separating into folds for training and testing

set.seed(99)
# only price is logged

atrial <- lm(log10(Price) ~., data = mxapartment)
summary(atrial) 
```

```{r}

# using a log10 on Price and Distance

adata <- mxapartment
adata$Distance <- adata$Distance + 2
adata$Distance <- log10(adata$Distance)

summary(adata)
atrial2 <- lm(log10(Price) ~., data = adata)
summary(atrial2) # improves on above model
```

```{r}
adata2 <- adata
adata2$BuildingArea <- log10(adata2$BuildingArea)

atrial3 <- lm(log10(Price) ~., data = adata2)
summary(atrial3) # improves above model

```

```{r}
# testing selection methods

# backwards selection
summary(step(atrial3, direction = 'backward', trace = 0)) 

```

```{r}
# forwards selection
summary(step(atrial3, direction = 'forward', trace = 0)) 
```

```{r}
# stepwise selection (both)
summary(step(atrial3, direction = 'both', trace = 0))

# forward selection in minimally more efficient here
# forward uses all 10 variables,
# backwards and step use 7 variables
```

```{r}

# Log10 of Price into data prior to setting training and testing sets

# log10 to Price

adata3 <- adata2
adata3$Price <- log10(adata2$Price)
glimpse(adata3) # verifying it was logged

```

```{r}
#correlation with log10 features

pairs.panels(adata3, cex.cor = 2) # with log10 data, cex.cor =2 makes correlation #'s larger

```

```{r}
# setting up training and test sets

set.seed(99)

# creating the index for the split to occur
aIndex <- sample(2, nrow(adata3), replace = T, prob = c(0.7, 0.3))


aTrain <- adata3[aIndex == 1, ] # create training set, 70%
aTest <- adata3[aIndex == 2, ] # create test set, 30%

library(caret)

# creating cross validation, 10 fold, ran 5 times
# control parameters

aControls <- trainControl(method = 'repeatedcv', number = 10,  repeats = 5, verboseIter = F) #verboseIter shows iterations running on screen
```

```{r}

# Linear Regression

aLinTrain <- train(Price ~., aTrain, method = 'lm', trControl = aControls)
aLinTrain$results
summary(aLinTrain)

```

```{r}
plot(aLinTrain$finalModel)

#seems to be an issue at residuals vs fitted is not horrible, see if it can be improved

```

```{r}
set.seed(99)


library(glmnet) 
# Setting up Ridge regression

aRidge <- train(Price ~., 
                  aTrain, 
                  method = 'glmnet', 
                  tuneGrid = expand.grid(alpha = 0,
                                         lambda = seq(0.0001, 1, length = 5)),
                  trControl = aControls)

```

```{r}
plot(aRidge) # Showing RMSE responding to increase in Lambda values
```

```{r}
print(aRidge)
```

```{r}
# plot showing importance of variables for Ridge Regression

plot(varImp(aRidge, scale = F)) 
```

```{r}

# Lasso Regression

set.seed(99)

aLasso <- train(Price ~., 
                         aTrain, 
                         method = 'glmnet', 
                         tuneGrid = expand.grid(alpha = 1,
                                                lambda = seq(0.0001, 1, length = 5)),
                         trControl = aControls)
```

```{r}
plot(aLasso) # lowest lambda is best for lasso as well
```

```{r}
plot(varImp(aLasso, scale = F)) # importance level of variables for Lasso
```

```{r}
plot(aLasso$finalModel, xvar = 'dev', label = T) # 8 variables explain 50%
```

```{r}
# Elastic Net

set.seed(99)

aElastic <- train(Price ~., 
                         aTrain, 
                         method = 'glmnet', 
                         tuneGrid = expand.grid(alpha = seq(0, 1, length =10),
                                                lambda = seq(0.0001, 1, length = 5)),
                         trControl = aControls)

```

```{r}
# best fit for Lamba and Alpha graph

plot(aElastic) #lambda is coloured lines, alpha along bottom, run again with length 3 for alpha for clarity

```

```{r}
aElastic$bestTune #finding optimal lambda & alpha values in a separate way

```

```{r}
# Comparing models - MAE, RMSE, Rsquared

aModelList <- list(Ridge = aRidge, Lasso = aLasso, Elastic = aElastic)
aResample <- resamples(aModelList)
summary(aResample)

#not allowing linear model due to resample size, inspect this
```

```{r}
# Predictions with Training & Test data
# all models performed closely however the Ridge & Elastic netregression have the best mean RMSE
# So Ridge will be used here, as RMSE is the main measure of this study


# training data prediction & RMSE
aP1 <- predict(aRidge, aTrain)
sqrt(mean((aTrain$Price - aP1)^2)) # RMSE - lower the better
```

```{r}
# Test data prediction RMSE
aP2 <- predict(aRidge, aTest)
sqrt(mean((aTest$Price - aP2)^2))

```

```{r}
# comparing accuracy

#comparing actual vs predicted values 
aCompare <- cbind(actual = aTest$Price, aP2) # combining actual and predicted

head(aCompare)

```

```{r}
# calculating accuracy
mean (apply(aCompare, 1, min)/apply(aCompare, 1, max))

# The model is very accurate
```

```{r}

# For the Log10(Distance variable) the percent increase is computed as follows

# (Distance1/ Distance2) ^ Distance Coefficient

```

```{r}
# Decision Tree

library(rpart)
set.seed(99)

aTree <- rpart(Price ~., data = aTrain, method = "anova")
summary(aTree)

```

```{r}
#RMSE for Decision Tree Training set

aTreeP1 <- predict(aTree, aTrain )
RMSE(aTreeP1, aTrain$Price)
```

```{r}
# done on test data

aTreeP2 <- predict(aTree, aTest )
RMSE(aTreeP2, aTest$Price)
```

```{r}
# Using random forest

library(randomForest)

set.seed(99)

aForestTry <- randomForest(Price~., data = aTrain)
attributes(aForestTry) # gives a list of attributes that can go after $
aForestTry$importance # gives importance of each variable

```

```{r}

aForestTry$type # regression

```

```{r}
plot(aForestTry, main = 'First Forest Attempt befor fine tuning') # graph on error as it reduces and becomes constant for number of trees

```

```{r}
# histogram of the number of nodes per tree
hist(treesize(aForestTry),
     main = '# of Nodes per Tree',
     col = 'gold')

```

```{r}
# graphical display of ForestTry$importance
varImpPlot(aForestTry, main = 'Variable Importance', col = 'darkorange') 
```

```{r}
# Number of times a variable appeared in random forest, printed in relation to the variable column number
varUsed(aForestTry)
```

```{r}
# print out of ForestTry and results
aForestTry
```

```{r}
# Predictino for RMSE on Train data

aPreTry <- predict(aForestTry, aTrain) # default
(RMSE(aPreTry, aTrain$Price))       # default
```

```{r}
# prediction RMSE for Test data

aPreTest <- predict(aForestTry, aTest)  
(RMSE(aPreTest, aTest$Price))
```

```{r}

# attempting to tune the Random Forest, finding best mtry and oob error

aTuneForest<- tuneRF(y = aTrain$Price, x = aTrain[,-1], 
                    mtryStart = 3, #using default, same as above to start
                    ntreeTry = 300, # on plot error seems to stabilize a 300
                    stepFactor = 1,
                    improve = 0.05,
                    trace = T,
                    plot = T,
                    doBest = T)
```

```{r}

aForest <- randomForest(Price ~., data = aTrain,
                          ntree = 500,
                           # default for regression
                          importance = T)

aForest

```

```{r}

plot(aForest)

```

```{r}

```

```{r}

# getting RMSE for training data

aForestP1 <- predict(aForest, aTrain)

plot(aForestP1)

```

```{r}
# RMSE for training data

RMSE(aForestP1, aTrain$Price)

```

```{r}
# plot for Test data

aForestP2 <- predict(aForest, aTest)
plot(aForestP2)

```

```{r}

# RMSE for Test data
RMSE(aForestP2, aTest$Price)

```

```{r}

```

```{r}

```





